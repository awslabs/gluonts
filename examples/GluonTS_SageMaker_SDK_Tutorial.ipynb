{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GluonTS SageMaker SDK Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***This notebook is meant to be uploaded to a SageMaker notebook instance and executed there. As a kernel choose `conda_mxnet_p36`***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***In this how-to tutorial we will train a SimpleFeedForwardEstimator on the m4_hourly dataset on AWS SageMaker using the GluonTSFramework, and later review its performance. At the very end you will see how to launch your custom training script.*** <br/>\n",
    "***In the end you should know how to train any GluonEstimator on any Dataset on SageMaker using the GluonTSFramework train(...) method, and how to run your own script using the run(...) method.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently, *GluonTSFramework* is only available through the master branch of *GluonTS*, so we install it with the required dependencies first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade mxnet==1.6  git+https://github.com/awslabs/gluon-ts.git#egg=gluonts[dev]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Third-party requirements\n",
    "import boto3\n",
    "import sagemaker\n",
    "from pathlib import Path\n",
    "import tempfile\n",
    "\n",
    "# First-party requirements\n",
    "from gluonts.nursery.sagemaker_sdk.estimator import GluonTSFramework\n",
    "from gluonts.model.simple_feedforward import SimpleFeedForwardEstimator\n",
    "from gluonts.dataset.repository.datasets import get_dataset, dataset_recipes\n",
    "from gluonts.mx.trainer import Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Credentials & Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are executing this tutorial on a SageMaker notebook instance, many parameters that we would usually need to predefine manually we can just retrieve from the environment. In order to highlight how you would have to set these parameters when you are executing a notebook like this on you local machine take a look at the cell output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_session = boto3.session.Session()\n",
    "temp_sagemaker_session =  sagemaker.session.Session(boto_session=temp_session)\n",
    "bucket_name = f\"s3://{temp_sagemaker_session.default_bucket()}\"\n",
    "print(f\"bucket_name = '{bucket_name}'\")\n",
    "region_name = temp_session.region_name\n",
    "print(f\"region_name = '{region_name}'\")\n",
    "profile_name = temp_session.profile_name\n",
    "print(f\"profile_name = '{profile_name}'\")\n",
    "iam_role = sagemaker.get_execution_role()\n",
    "print(f\"iam_role = '{iam_role}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that in order to be able to use the profile 'defult' (or any other profile) on your local machine you must have correctly set up your [AWS CLI](https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2.html). Additionally, the specified bucket needs to be actually present in the specified region. With this out of the way, we can continue as if we had set the above variables manually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimental Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we should define the *S3 parent folder location* which will later contain the folder with all the data generated during the experiment (model artifacts, custom scripts, dependencies etc.). I you choose to use a subfolder for your experiments (like we do here) the folder does not have to exist yet, but it's name must satisfy the regular expression pattern: \\^\\[a-zA-Z0-9\\](-\\*\\[a-zA-Z0-9\\])\\*. If not specified, the default bucket of the specified region itself will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_parent_dir = bucket_name + \"/my-sagemaker-experiments\"\n",
    "print(f\"experiment_parent_dir = '{experiment_parent_dir}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SageMaker session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to create a sagemaker session in our region using a [*boto3*](https://boto3.amazonaws.com/v1/documentation/api/latest/guide/quickstart.html#using-boto-3) session with our credentials (profile)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boto_session = boto3.session.Session(profile_name=profile_name, region_name=region_name)\n",
    "sagemaker_session =  sagemaker.session.Session(boto_session=boto_session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AWS IAM role"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to provide an AWS [IAM](https://docs.aws.amazon.com/IAM/latest/UserGuide/introduction.html) role, with which to access the resources on our account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "role = iam_role"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training image & instance type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can just use one of the prebuilt SageMaker [ECR](https://docs.aws.amazon.com/AmazonECR/latest/userguide/docker-basics.html) images and install the gluonts version we prefer dynamically though the 'requirements.txt'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_instance_type = \"cpu\" \n",
    "# instance_type = \"gpu\" # alternative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending our *general_instance_type* choice we will have to select an appropriate concrete 'instance type':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_type = \"ml.c5.xlarge\" if general_instance_type == \"cpu\" else \"ml.p2.xlarge\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and an appropriate prebuilt mxnet image (we will take the training images here):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if general_instance_type == \"cpu\":\n",
    "    docker_image = f\"763104351884.dkr.ecr.{region_name}.amazonaws.com/mxnet-training:1.6.0-cpu-py36-ubuntu16.04\"\n",
    "else:\n",
    "    docker_image = f\"763104351884.dkr.ecr.{region_name}.amazonaws.com/mxnet-training:1.6.0-gpu-py36-cu101-ubuntu16.04\"\n",
    "print(f\"docker_image = '{docker_image}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base job description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can give our training job a base name that lets us easily identify experiments of the same type. <br/>\n",
    "It has to satisfy the regular expression pattern: \\^\\[a-zA-Z0-9\\](-\\*\\[a-zA-Z0-9\\])\\*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_job_description = \"my-sagemaker-experiment-intro\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have two choices; we can either pick a built in dataset provided by GluonTS or any dataset in the GluonTS dataset format located on S3, which would look like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">dataset_name<br/>\n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;|---> train<br/>\n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;|--> data.json<br/>\n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;|---> test<br/>\n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;|--> data.json<br/>\n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;|---> metadata.json<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we haven't uploaded any, lets pick a provided one for now. <br/>\n",
    "The following datasets are available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset_recipes.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How about \"m4_hourly\"?:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"m4_hourly\"\n",
    "# dataset_name = \"s3://<your-custom-dataset-location>\" # if using a custom dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need to know the *prediction_length* and *freq* of the dataset to define our SimpleFeedForwardEstimator, so lets keep track of them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = dataset_recipes[dataset_name].keywords[\"pandas_freq\"]\n",
    "prediction_length = dataset_recipes[dataset_name].keywords[\"prediction_length\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requirements and Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will additionally have to specify a 'requirements.txt' file where we specify which GluonTS version we want to use. <br/>\n",
    "Here we will create a temporary requirements file, but you can just have a 'requirements.txt' file in the folder where you launch your experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "requirements_dot_txt_file_name = \"requirements.txt\"\n",
    "requirements_dot_txt_file_content = \"git+https://github.com/awslabs/gluon-ts.git\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only using temporary directory for demonstration\n",
    "temp_dir = tempfile.TemporaryDirectory()\n",
    "temp_dir_path = Path(temp_dir.name)\n",
    "\n",
    "# create the requirements.txt file\n",
    "with open(temp_dir_path / requirements_dot_txt_file_name, \"w\") as req_file: # has to be called requirements.txt\n",
    "    req_file.write(requirements_dot_txt_file_content)\n",
    "my_requirements_txt_file_path = str(temp_dir_path / requirements_dot_txt_file_name)\n",
    "print(f\"my_requirements_txt_file_path = '{my_requirements_txt_file_path}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define the Estimator we want to train, which can be any GluonEstimator (except ) with any hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_estimator = SimpleFeedForwardEstimator(\n",
    "                    prediction_length=prediction_length,\n",
    "                    freq=freq,\n",
    "                    trainer=Trainer(ctx=general_instance_type, epochs=5) # optional\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Launch the Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_experiment = GluonTSFramework(\n",
    "                    sagemaker_session=sagemaker_session,\n",
    "                    role=role,\n",
    "                    image_name=docker_image,  \n",
    "                    base_job_name=base_job_description,\n",
    "                    train_instance_type=instance_type,\n",
    "                    dependencies=[my_requirements_txt_file_path],\n",
    "                    output_path=experiment_parent_dir, # optional, but recommended\n",
    "                    code_location=experiment_parent_dir, # optional, but recommended\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally we call the *train* method to train our estimator, where we just specify our dataset and estimator:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results = my_experiment.train(dataset=dataset_name, estimator=my_estimator) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review the Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 'train(...)' function returnes a TrainResult which consists of the following fields:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results._fields)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we could use the predictor straight away to predict on some additional data if we would like. <br/>\n",
    "We can also inspect our training history and monitored metrics (like resource consumption or epoch loss) on SageMaker under \"Training/Training jobs\" here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"https://{region_name}.console.aws.amazon.com/sagemaker/home?region={region_name}#/jobs/{results.job_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or take a look at the metrics right here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.metrics[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or head to our bucket to download the model artifacts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"https://s3.console.aws.amazon.com/s3/buckets/{experiment_parent_dir[5:]}/{results.job_name}/?region={region_name}&tab=overview\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run a custom python script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There process to run a custom python script is not much different, however, you will have to adapt your usual python script to particularities of the SageMaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gluonts\n",
    "import s3fs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing a custom script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your custom script has to adhere to a rough format, for this reason we provide the \"run_entry_point.py\" script with GluonTS under:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_entry_point_path = (\n",
    "    Path(os.path.dirname(gluonts.__file__))\n",
    "    / \"nursery\"\n",
    "    / \"sagemaker_sdk\"\n",
    "    / \"entry_point_scripts\"\n",
    "    / \"run_entry_point.py\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets take a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(run_entry_point_path, 'r') as script:\n",
    "    print(script.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, there is a *run* method, whithin which we are supposed to write our custom code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, at the bottom we might need to parse additional arguments that we provide for example through the \"inputs\" parameter of the GluonTSFramework.run(...) method. The \"inputs\" parameter cannot be empty, due to the restrictions of the Framework baseclass of the GluonTSFramework, however, you can pass an empty file located on S3 as dummy input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets define a path for the dummy file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_s3_file_path = bucket_name + \"/dummy_1234\"\n",
    "print(f\"dummy_s3_file_path = '{dummy_s3_file_path}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets create the S3 file (if the file already exists you will have to set overwrite to 'True', or choose a different path for the dummy file):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overwrite = False\n",
    "s3 = s3fs.S3FileSystem(anon=False)  # uses default credentials\n",
    "if not(s3.exists(dummy_s3_file_path)) or overwrite:\n",
    "    with s3.open(dummy_s3_file_path, 'w') as f:\n",
    "        f.write(\"This is a dummy file.\")  \n",
    "    print(\"Dummy file created!\")\n",
    "else:\n",
    "    print(\"No dummy file created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_inputs = {'my_dataset_name': sagemaker.s3_input(dummy_s3_file_path, content_type='application/json')} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we were to pass a dataset location as input as defined above, we would have to parse the location of that dataset (which will be uploaded into the container environment) for example like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> parser.add_argument('--my_fancy_dataset', type=str, default=os.environ['SM_CHANNEL_MY_DATASET_NAME'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepending \"SM_CHANNEL_\" and converting the name to all caps is necessary. <br/>\n",
    "Within the *run(...)* method the location will be accessible by:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> arguments.my_fancy_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any additional \"hyperparameter\" you provide to *GluonTSFramework.run(...)* are already parsed by:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">parser.add_argument(\"--sm-hps\", type=json.loads, default=os.environ[\"SM_HPS\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get familiar tasks:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, we will only use the unmodified run script, however, a good exercise to get familiar with the framework would be to modify the script so:\n",
    "* You parse the location of the input we provide thourgh \"my_inputs\" \n",
    "* You read the dummy file inside the run(...) method\n",
    "* You write the content of the file to a new file called \"parsed.txt\" and save it to the output location \n",
    "* You check in S3 that \"parsed.txt\" was saved to S3 in your experiment folder under /output/output.tar.gz\n",
    "\n",
    "HINT: you don't need to write or read form S3 explicitly, but rather access the appropriate local location through \"arguments\" of the run(...) method within your scripts; let SageMaker containers handle the interaction with S3. <br/>\n",
    "HINT: you can take a look at the \"train_entry_point.py\" to see an actual example for a training script."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we will see, the arguments to the GluonTSFramework run(...) method are almost identical to the train(...) one, however, we additionally specify the required \"entry_point\" and \"inputs\", and optionally \"wait=False\" because we might want to launch multiple jobs async."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_experiment, my_job_name = GluonTSFramework.run(\n",
    "                    entry_point=str(run_entry_point_path), # additionally required\n",
    "                    inputs = my_inputs, # additionally required\n",
    "                    sagemaker_session=sagemaker_session,\n",
    "                    role=role,\n",
    "                    image_name=docker_image,  \n",
    "                    base_job_name=base_job_description,\n",
    "                    train_instance_type=instance_type,\n",
    "                    dependencies=[my_requirements_txt_file_path],\n",
    "                    output_path=experiment_parent_dir, # optional, but recommended\n",
    "                    code_location=experiment_parent_dir, # optional, but recommended\n",
    "                    wait=False # optional\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can take a look at the training job right away:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"https://{region_name}.console.aws.amazon.com/sagemaker/home?region={region_name}#/jobs/{my_job_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And again, check out the corresponding S3 location:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"https://s3.console.aws.amazon.com/s3/buckets/{experiment_parent_dir[5:]}/{my_job_name}/?region={region_name}&tab=overview\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom GluonTS version:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case you are modifying GluonTS on your local machine and want to run experiments on your custom version, just import GluonTS and define:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">gluont_ts_path = Path(gluonts.__path__[0]) <br/>\n",
    ">gluont_ts_requirements_path = gluont_ts_path.parent.parent / \"requirements\" / \"requirements.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and change the dependencies argument of run(...) or train(...) the following way:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> dependencies=[gluont_ts_requirements_path, gluont_ts_path]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets just clean up the temporary directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_dir.cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
