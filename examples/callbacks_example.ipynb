{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GluonTS Callbacks\n",
    "This notebook illustrates how one can control the training with GluonTS Callback's. A callback is a function which gets called at one or more specific hook points during training.\n",
    "You can use predefined GluonTS callbacks like the logging callback TrainingHistory, ModelAveraging or TerminateOnNaN, or you can implement your own callback."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Using a single Callback\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetching some data\n",
    "from gluonts.dataset.repository.datasets import get_dataset\n",
    "\n",
    "dataset = \"m4_hourly\"\n",
    "dataset = get_dataset(dataset)\n",
    "prediction_length = dataset.metadata.prediction_length\n",
    "freq = dataset.metadata.freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gluonts.model.simple_feedforward import SimpleFeedForwardEstimator\n",
    "from gluonts.mx.trainer import Trainer\n",
    "from gluonts.mx.trainer.callback import TrainingHistory\n",
    "\n",
    "# defining a callback, which will log the training loss for each epoch\n",
    "history = TrainingHistory()\n",
    "\n",
    "trainer=Trainer(epochs=20, callbacks=history)\n",
    "estimator = SimpleFeedForwardEstimator(prediction_length=prediction_length, freq = freq, trainer=trainer)\n",
    "\n",
    "predictor = estimator.train(dataset.train, num_workers=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the training loss over the epochs\n",
    "print(history.loss_history)\n",
    "\n",
    "# in case you are using a validation dataset you can get the validation loss with \n",
    "# history.validation_loss_history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Using multiple Callbacks\n",
    "To continue the training from a given predictor you can use the WarmStart Callback. When you want to use more than one callback, provide them as a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gluonts.mx.trainer.callback import WarmStart\n",
    "\n",
    "warm_start = WarmStart(predictor=predictor)\n",
    "\n",
    "trainer=Trainer(epochs=10, callbacks=[history, warm_start])\n",
    "\n",
    "estimator = SimpleFeedForwardEstimator(prediction_length=prediction_length, freq = freq, trainer=trainer)\n",
    "\n",
    "predictor = estimator.train(dataset.train, num_workers=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(history.loss_history) # The training loss history of all 20+10 epochs we trained the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Default Callbacks\n",
    "In addition to the Callbacks you specify, the GluonTS Trainer uses the two default Callbacks ModelAveraging and LearningRateReduction. You can turn them off by setting add_default_callbacks=False when initializing the Trainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer=Trainer(epochs=20, callbacks=history) # use the TrainingHistory Callback and the default callbacks.\n",
    "trainer=Trainer(epochs=20, callbacks=history, add_default_callbacks=False) # use only the TrainingHistory Callback\n",
    "trainer=Trainer(epochs=20, add_default_callbacks=False) # use no callback at all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Custom Callbacks\n",
    "To implement your own Callback you can write a class which inherits from the GluonTS Callback class and overwrite one or more of the hooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Have a look at the abstract Callback class, the hooks take different arguments which you can use.\n",
    "# Hook methods with boolean return value stop the training if False is returned.\n",
    "\n",
    "from gluonts.mx.trainer.callback import Callback\n",
    "import inspect\n",
    "lines = inspect.getsource(Callback)\n",
    "print(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is an example implementation of a Metric value based early stopping custom callback implementation\n",
    "# it only implements the hook method \"on_epoch_end()\"\n",
    "# which gets called after all batches of one epoch have been processed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gluonts.evaluation import Evaluator\n",
    "from gluonts.dataset.common import Dataset\n",
    "from gluonts.mx.model.predictor import GluonPredictor\n",
    "from mxnet.gluon import nn\n",
    "from mxnet import gluon\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "from gluonts.support.util import copy_parameters\n",
    "\n",
    "class MetricInferenceEarlyStopping(Callback):\n",
    "    \"\"\"\n",
    "    Early Stopping mechanism based on the prediction network.\n",
    "    Can be used to base the Early Stopping directly on a metric of interest, instead of on the training/validation loss.\n",
    "    In the same way as test datasets are used during model evaluation,\n",
    "    the time series of the validation_dataset can overlap with the train dataset time series,\n",
    "    except for a prediction_length part at the end of each time series.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    validation_dataset\n",
    "        An out-of-sample dataset which is used to monitor metrics\n",
    "    predictor\n",
    "        A gluon predictor, with a prediction network that matches the training network\n",
    "    evaluator\n",
    "        The Evaluator used to calculate the validation metrics.\n",
    "    metric\n",
    "        The metric on which to base the early stopping on.\n",
    "    patience\n",
    "        Number of epochs to train on given the metric did not improve more than min_delta.\n",
    "    min_delta\n",
    "        Minimum change in the monitored metric counting as an improvement\n",
    "    verbose\n",
    "        Controls, if the validation metric is printed after each epoch.\n",
    "    minimize_metric\n",
    "        The metric objective.\n",
    "    restore_best_network\n",
    "        Controls, if the best model, as assessed by the validation metrics is restored after training.\n",
    "    num_samples\n",
    "        The amount of samples drawn to calculate the inference metrics.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        validation_dataset: Dataset,\n",
    "        predictor: GluonPredictor,\n",
    "        evaluator: Evaluator = Evaluator(num_workers=None),\n",
    "        metric: str = \"MSE\",\n",
    "        patience: int = 10,\n",
    "        min_delta: float = 0.0,\n",
    "        verbose: bool = True,\n",
    "        minimize_metric: bool = True,\n",
    "        restore_best_network: bool = True,\n",
    "        num_samples: int = 100,\n",
    "    ):\n",
    "        assert (\n",
    "            patience >= 0\n",
    "        ), \"EarlyStopping Callback patience needs to be >= 0\"\n",
    "        assert (\n",
    "            min_delta >= 0\n",
    "        ), \"EarlyStopping Callback min_delta needs to be >= 0.0\"\n",
    "        assert (\n",
    "            num_samples >= 1\n",
    "        ), \"EarlyStopping Callback num_samples needs to be >= 1\"\n",
    "\n",
    "        self.validation_dataset = list(validation_dataset)\n",
    "        self.predictor = predictor\n",
    "        self.evaluator = evaluator\n",
    "        self.metric = metric\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.verbose = verbose\n",
    "        self.restore_best_network = restore_best_network\n",
    "        self.num_samples = num_samples\n",
    "\n",
    "        if minimize_metric:\n",
    "            self.best_metric_value = np.inf\n",
    "            self.is_better = np.less\n",
    "        else:\n",
    "            self.best_metric_value = -np.inf\n",
    "            self.is_better = np.greater\n",
    "\n",
    "        self.validation_metric_history: List[float] = []\n",
    "        self.best_network = None\n",
    "        self.n_stale_epochs = 0\n",
    "\n",
    "    def on_epoch_end(\n",
    "        self,\n",
    "        epoch_no: int,\n",
    "        epoch_loss: float,\n",
    "        training_network: nn.HybridBlock,\n",
    "        trainer: gluon.Trainer,\n",
    "        best_epoch_info: dict,\n",
    "        ctx: mx.Context\n",
    "    ) -> bool:\n",
    "        should_continue = True\n",
    "        copy_parameters(training_network, self.predictor.prediction_net)\n",
    "\n",
    "        from gluonts.evaluation.backtest import make_evaluation_predictions\n",
    "\n",
    "        forecast_it, ts_it = make_evaluation_predictions(\n",
    "            dataset=self.validation_dataset,\n",
    "            predictor=self.predictor,\n",
    "            num_samples=self.num_samples,\n",
    "        )\n",
    "\n",
    "        agg_metrics, item_metrics = self.evaluator(ts_it, forecast_it)\n",
    "        current_metric_value = agg_metrics[self.metric]\n",
    "        self.validation_metric_history.append(current_metric_value)\n",
    "\n",
    "        if self.verbose:\n",
    "            print(\n",
    "                f\"Validation metric {self.metric}: {current_metric_value}, best: {self.best_metric_value}\"\n",
    "            )\n",
    "\n",
    "        if self.is_better(current_metric_value, self.best_metric_value):\n",
    "            self.best_metric_value = current_metric_value\n",
    "\n",
    "            if self.restore_best_network:\n",
    "                training_network.save_parameters(\"best_network.params\")\n",
    "\n",
    "            self.n_stale_epochs = 0\n",
    "        else:\n",
    "            self.n_stale_epochs += 1\n",
    "            if self.n_stale_epochs == self.patience:\n",
    "                should_continue = False\n",
    "                print(\n",
    "                    f\"EarlyStopping callback initiated stop of training at epoch {epoch_no}.\"\n",
    "                )\n",
    "\n",
    "                if self.restore_best_network:\n",
    "                    print(\n",
    "                        f\"Restoring best network from epoch {epoch_no - self.patience}.\"\n",
    "                    )\n",
    "                    training_network.load_parameters(\"best_network.params\")\n",
    "\n",
    "        return should_continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the custom callback\n",
    "\n",
    "from gluonts.dataset.repository.datasets import get_dataset\n",
    "from gluonts.model.simple_feedforward import SimpleFeedForwardEstimator\n",
    "from gluonts.mx.trainer import Trainer\n",
    "\n",
    "dataset = \"m4_hourly\"\n",
    "dataset = get_dataset(dataset)\n",
    "prediction_length = dataset.metadata.prediction_length\n",
    "freq = dataset.metadata.freq\n",
    "\n",
    "estimator = SimpleFeedForwardEstimator(prediction_length=prediction_length, freq = freq)\n",
    "training_network = estimator.create_training_network()\n",
    "transformation = estimator.create_transformation()\n",
    "\n",
    "predictor = estimator.create_predictor(transformation=transformation, trained_network=training_network)\n",
    "\n",
    "es_callback = MetricInferenceEarlyStopping(validation_dataset=dataset.test, predictor=predictor, metric=\"MSE\")\n",
    "\n",
    "trainer = Trainer(epochs=200, callbacks=es_callback)\n",
    "\n",
    "estimator.trainer = trainer\n",
    "\n",
    "pred = estimator.train(dataset.train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
