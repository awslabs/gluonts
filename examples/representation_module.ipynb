{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I/O Representations in GluonTS\n",
    "\n",
    "GluonTS now includes a representation module which can be used to transform both input and outputs. More info can be found here:\n",
    "- Paper: https://arxiv.org/abs/2005.10111\n",
    "- GluonTS representation documentation: https://gluon-ts.s3-accelerate.dualstack.amazonaws.com/master/api/gluonts/gluonts.representation.html\n",
    "- GluonTS representation source code: https://github.com/awslabs/gluon-ts/tree/master/src/gluonts/mx/representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard imports\n",
    "\n",
    "We first import a bunch of libraries ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import mxnet as mx\n",
    "from mxnet import gluon\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load a dataset\n",
    "\n",
    "... and then we load the `m4_hourly` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from gluonts.dataset.repository.datasets import get_dataset\n",
    "dataset = get_dataset(\"m4_hourly\", regenerate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the representation module\n",
    "\n",
    "Neural time series models (and in fact many other deep learning based models) are highly sensitive to the representation of their inputs and outputs. To tackle this issue, GluonTS now enables the user to flexibly define which input and output transformation they want the underlying model to use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Available I/O representations\n",
    "\n",
    "GluonTS provides a few popular representation transformations out-of-the-box:\n",
    "- `Representation`: This is the base class for the representation module. This representation will not change the data in any way, i.e. it corresponds to an identity transformation. \n",
    "    - Example: `Representation()`.\n",
    "- `MeanScaling`: This representation scales all time series by their mean. \n",
    "    - Example: `MeanScaling()`.\n",
    "- `GlobalRelativeBinning`: This representation first scales all time series by their mean and then bins all time series using a single, hence global, binning. Users can further specifiy the binning resolution as well as whether bins should be equally spaced or quantile-based.\n",
    "    - Example: `GlobalRelativeBinning(num_bins=100, is_quantile=True)`.\n",
    "- `LocalAbsoluteBinning`: This representation bins each time series individually, thereby implicitly scaling them. Again, users can specifiy the binning resolution as well as whether bins should be equally spaced or quantile-based. \n",
    "     - Example: `LocalAbsoluteBinning(num_bins=50, is_quantile=False)`.\n",
    "- `CustomBinning`: This representation bins each time series using the provided bin centers (in contrast to the previously presented binning strategies in which the bin centers are automatically computed). \n",
    "    - Example: `CustomBinning(bin_centers=np.linspace(10,1000,50))`.\n",
    "- `DimExpansion`: This representation expands the incoming data tensor on a desired axis. This is usually needed to expand a scaled representation (usually on axis=1) to be passed to the model. Models usually expect 3-dimensional tensors in (N,C,T) format, where N = number of samples (batch size), C = number of features (dimensionality), and T = number of time steps. \n",
    "    - Example: `DimExpansion()`.\n",
    "- `Embedding`: This representation embeds the incoming data tensor. \n",
    "    - Example: `Embedding(num_bins=1024)`.\n",
    "- `ConstNormalization`: This representation applies a normalization by a constant to an incoming data tensor. \n",
    "    - Example: `ConstNormalization(const=1024)`.\n",
    "- `RepresentationChain`: This representation chains multiple representations, i.e. multiple representations can be applied on top of each other. This is useful for expanding a dimension on top of mean-scaled date, or for applying the discrete probability integral transform on top of a quantile-based binning. Representations are applied from left to right. \n",
    "    - Examples: \n",
    "        - `RepresentationChain(chain=[MeanScaling(),DimExpansion()])`\n",
    "        - `RepresentationChain(chain=[LocalAbsoluteBinning(num_bins=1024), Embedding(num_bins=1024)])`\n",
    "        - `RepresentationChain(chain=[GlobalRelativeBinning(num_bins=1024), ConstNormalization(num_bins=1024), DimExpansion()])`\n",
    "- `HybridRepresentation`: This representation stacks multiple representations, i.e. multiple representations can be concatenated (on axis=1). This is especially useful for passing binnings at multiple scales. \n",
    "    - Examples: \n",
    "        - `HybridRepresentation(representations=[RepresentationChain(chain=[GlobalRelativeBinning(num_bins=16), Embedding(num_bins=16)]), RepresentationChain(chain=[GlobalRelativeBinning(num_bins=128), Embedding(num_bins=128)]), RepresentationChain(chain=[GlobalRelativeBinning(num_bins=1024), Embedding(num_bins=1024)])])`\n",
    "        - `HybridRepresentation(representations=[RepresentationChain(chain=[GlobalRelativeBinning(num_bins=1024), Embedding(num_bins=1024)]),RepresentationChain(chain=[LocalAbsoluteBinning(num_bins=1024), Embedding(num_bins=1024)])])`\n",
    "\n",
    "Please consult the documentation for a complete list of constructor parameters. See <a href=\"#Creating-your-own-representations\">below</a> on how to create your own representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gluonts.mx.representation import (\n",
    "    Representation, \n",
    "    MeanScaling, \n",
    "    GlobalRelativeBinning, \n",
    "    DimExpansion, \n",
    "    Embedding, \n",
    "    RepresentationChain,\n",
    "    ConstNormalization\n",
    ")\n",
    "from gluonts.mx.distribution import StudentTOutput, CategoricalOutput\n",
    "\n",
    "binning = False\n",
    "\n",
    "if binning:\n",
    "    input_repr = RepresentationChain(chain=[GlobalRelativeBinning(num_bins=1024), Embedding(num_bins=1024)])\n",
    "    output_repr = GlobalRelativeBinning(num_bins=1024)\n",
    "    distr_output = CategoricalOutput(num_cats=1024)\n",
    "else:\n",
    "    input_repr = RepresentationChain(chain=[MeanScaling(), DimExpansion()])\n",
    "    output_repr = Representation()\n",
    "    distr_output = StudentTOutput()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define and train an estimator\n",
    "\n",
    "The following estimators currently support varibale I/O representations ([PR on GitHub](https://github.com/awslabs/gluon-ts/pull/840)):\n",
    "- `DeepAREstimator`\n",
    "- `SimpleFeedForwardEstimator`\n",
    "- `TransformerEstimator`\n",
    "- `WaveNetEstimator`\n",
    "\n",
    "For these estimators, users can now specify both  the `input_repr` and the `output_repr` respectively.\n",
    "\n",
    "Please consult the documentation for a complete list of constructor parameters. See <a href=\"#Updating-a-model-to-support-the-representation-module\">below</a> on how adapt an estimator to support the representation module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from gluonts.model.simple_feedforward import SimpleFeedForwardEstimator\n",
    "from gluonts.mx.trainer import Trainer\n",
    "\n",
    "estimator = SimpleFeedForwardEstimator(\n",
    "    num_hidden_dimensions=[10],\n",
    "    prediction_length=dataset.metadata.prediction_length,\n",
    "    context_length=100,\n",
    "    freq=dataset.metadata.freq,\n",
    "    input_repr=input_repr,\n",
    "    output_repr=output_repr,\n",
    "    distr_output=distr_output,\n",
    "    trainer=Trainer(\n",
    "        ctx=\"gpu\",\n",
    "        epochs=10,\n",
    "        learning_rate=1e-2,\n",
    "        num_batches_per_epoch=100,\n",
    "        hybridize=False\n",
    "    )\n",
    ")\n",
    "\n",
    "predictor = estimator.train(dataset.train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate evaluation predictions & evaluate\n",
    "\n",
    "Standard code for generating and evaluating predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gluonts.evaluation.backtest import make_evaluation_predictions\n",
    "forecast_it, ts_it = make_evaluation_predictions(\n",
    "    dataset=dataset.test,\n",
    "    predictor=predictor,\n",
    "    num_samples=100,\n",
    ")\n",
    "\n",
    "from gluonts.evaluation import Evaluator\n",
    "forecasts = list(forecast_it)\n",
    "tss = list(ts_it)\n",
    "evaluator = Evaluator(quantiles=[0.1, 0.5, 0.9])\n",
    "agg_metrics, item_metrics = evaluator(iter(tss), iter(forecasts), num_series=len(dataset.test))\n",
    "print(json.dumps(agg_metrics, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example transformation inspection\n",
    "\n",
    "Let's take a biref look at some example transformations and see how the data is transformed along the way. We first set up the representations that we want and some sample data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = 8\n",
    "\n",
    "ms = MeanScaling()\n",
    "grb = GlobalRelativeBinning(num_bins=bins)\n",
    "const = ConstNormalization(const=bins)\n",
    "emb = Embedding(num_bins=bins)\n",
    "\n",
    "data = np.array([\n",
    "    np.ones(10),\n",
    "    np.linspace(0, 9, 10),\n",
    "    np.sin(np.linspace(-np.pi, np.pi, 10)),\n",
    "    4 * np.cos(np.linspace(-np.pi, np.pi, 10)),\n",
    "    [4, 10, 8, 7, 6, 10, 8.5, 8 , 6, 10]\n",
    "])\n",
    "\n",
    "ms.initialize_from_array(data, mx.context.cpu())\n",
    "grb.initialize_from_array(data, mx.context.cpu())\n",
    "const.initialize_from_array(data, mx.context.cpu())\n",
    "\n",
    "emb.collect_params().initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the sample date first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data_mx = mx.nd.array(data)\n",
    "print(data_mx)\n",
    "plt.plot(data_mx.asnumpy().T)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `MeanScaling`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms_transf, _, _ = ms(data_mx, mx.nd.ones_like(data_mx), None, [])\n",
    "print(ms_transf)\n",
    "plt.plot(ms_transf.asnumpy().T)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `GlobalRelativeBinning`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grb_transf, _, _ = grb(data_mx, mx.nd.ones_like(data_mx), None, [])\n",
    "print(grb_transf)\n",
    "plt.plot(grb_transf.asnumpy().T)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `ConstNormalization` normalization on top of `GlobalRelativeBinning`-transformed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "const_transf, _, _ = const(grb_transf, mx.nd.ones_like(data_mx), None, [])\n",
    "print(const_transf)\n",
    "plt.plot(const_transf.asnumpy().T)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Embedding` on top of `GlobalRelativeBinning`-transformed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "emb_transf, _, _ = emb(grb_transf, mx.nd.ones_like(data_mx), None, [])\n",
    "print(emb_transf)\n",
    "for i in range(len(emb_transf)):\n",
    "    plt.scatter(emb_transf[i,:,0].asnumpy(), emb_transf[i,:,1].asnumpy(), alpha=0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating your own representations\n",
    "\n",
    "If you want to implement your own representations you must subclass from `gluonts.mx.representation.Representation` and implement the defined methods. Specifically, you must implement:\n",
    "- `hybrid_forward`: Here, you need to implement the forward transformation, given `F` (the used MXNet framework), the `data`, the `observed_indicator`, the `scale`, and a list of `rep_params`. If you are implementing a scaler, for instance, then you need to scale the data here. As return parameters, you need to pass the re-represented data, pass the scale of the data (note that you always need to pass this, even if you don't implement a scaler, as this is used by the network as an additional feature), and some additional representation parameters which can be passed to `post_transform` in the case that the representation is used on the output (most representations don't need this, see `LocalAbsoluteBinning` for an example how this parameter can be used). Note that you are able to use NumPy in this call since gradients for the transformed inputs are usually not required and no gradient computation is blocked.\n",
    "- `post_transform`: Here, you need to implement the backward transformation, given `F` (the used MXNet framework), `samples` from a distribution, the `scale`, and a list of `rep_params`.\n",
    "\n",
    "Optionally, you can also implement the following methods:\n",
    "- `initialize_from_dataset`: Here, you can specify a set of instruction to be computed on the complete dataset. For example, `GlobalRelativeBinning` uses `initialize_from_dataset` to compute a global binning on the entire training set.\n",
    "- `initialize_from_array` : Same as above, but with a complete NumPy array instead of a GluonTS dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now implement a simple median scaler `MedianScaling` as an alternative to the mean scaler. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gluonts.core.component import validated\n",
    "from gluonts.model.common import Tensor\n",
    "from typing import Tuple, Optional, List\n",
    "\n",
    "class MedianScaling(Representation):\n",
    "    \n",
    "    @validated()\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "    \n",
    "    def hybrid_forward(\n",
    "        self,\n",
    "        F,\n",
    "        data: Tensor,\n",
    "        observed_indicator: Tensor,\n",
    "        scale: Optional[Tensor],\n",
    "        rep_params: List[Tensor],\n",
    "        **kwargs,\n",
    "    ) -> Tuple[Tensor, Tensor, List[Tensor]]:\n",
    "        if scale is None:\n",
    "            data_np = data.asnumpy()\n",
    "            observed_np = observed_indicator.asnumpy()\n",
    "            observed_np[observed_np == 0] = np.nan\n",
    "            data_np = data_np * observed_np\n",
    "            scale = np.nanmedian(data_np, axis=-1)\n",
    "            scale = np.expand_dims(scale, axis=-1)\n",
    "            scale = F.array(scale)\n",
    "        scaled_data = F.broadcast_div(data, scale)\n",
    "        return scaled_data, scale, []\n",
    "        \n",
    "    def post_transform(\n",
    "        self, F, samples: Tensor, scale: Tensor, rep_params: List[Tensor]\n",
    "    ) -> Tensor:\n",
    "        transf_samples = F.broadcast_mul(samples, scale)\n",
    "        return transf_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Updating a model to support the representation module\n",
    "\n",
    "If you want to adapt a GluonTS module to support the representation module, you need to perform a couple of changes in both the `_estimator` and the `_network` files.\n",
    "\n",
    "In `_network`:\n",
    "- Add `input_repr` and `output_repr`, both of type `Representation`, as initialization parameters to both the training and the prediction network.\n",
    "- In the training network: In `hybrid_forward`, make sure to call `self.input_repr` and `self.output_repr` on the `past_target` and `future_target` respectively. Pass `None` as scale and an empty list `[]` as repr_params. Pass the scale returned by the input representation to the distribution if an actual distribution is used for loss calculation (not used in this example). Then make sure that the network receives the input-transformed values as inputs and calculates the loss using the output-transformed values.\n",
    "- In the prediction network: In `hybrid_forward`, first perform the same input and output transformations as in the training network. Then, using direct model predictions or samples from a distribution, `post_transform` the data and return the back-transformed predictions. Note that scaled distributions automatically sample from the correct scale, thereby removing the need for calling `post_transform` (see <a href=\"#Dealing-with-scales-in-distributions\">below</a>) and that auto-regressive models need a little more attention (see <a href=\"Dealing-with-auto-regressive-models\">below</a>)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyTrainNetwork(gluon.HybridBlock):\n",
    "    \n",
    "    def __init__(self, prediction_length, input_repr, output_repr, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.prediction_length = prediction_length\n",
    "        self.input_repr = input_repr\n",
    "        self.output_repr = output_repr\n",
    "\n",
    "        with self.name_scope():\n",
    "            self.nn = mx.gluon.nn.HybridSequential()\n",
    "            self.nn.add(mx.gluon.nn.Dense(units=50, activation='relu'))\n",
    "            self.nn.add(mx.gluon.nn.Dense(units=50, activation='relu'))\n",
    "            self.nn.add(mx.gluon.nn.Dense(units=self.prediction_length, activation='softrelu'))\n",
    "\n",
    "    def hybrid_forward(self, F, past_target, future_target):\n",
    "        input_tar_repr, scale, _ = self.input_repr(\n",
    "            past_target, F.ones_like(past_target), None, []\n",
    "        )\n",
    "        output_tar_repr, _, _ = self.output_repr(\n",
    "            future_target, F.ones_like(future_target), None, []\n",
    "        )\n",
    "        \n",
    "        prediction = self.nn(input_tar_repr)\n",
    "        \n",
    "        return (prediction - output_tar_repr).abs().mean(axis=-1)\n",
    "\n",
    "\n",
    "class MyPredNetwork(MyTrainNetwork):\n",
    "    \n",
    "    def hybrid_forward(self, F, past_target):\n",
    "        input_tar_repr, scale, _ = self.input_repr(\n",
    "            past_target, F.ones_like(past_target), None, []\n",
    "        )\n",
    "        _, _, rep_params = self.output_repr(\n",
    "            past_target, F.ones_like(past_target), None, []\n",
    "        )\n",
    "        prediction = self.nn(input_tar_repr)\n",
    "        \n",
    "        prediction = self.output_repr.post_transform(\n",
    "            F, prediction, scale, rep_params\n",
    "        )\n",
    "        return prediction.expand_dims(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `_estimator`:\n",
    "- Add `input_repr` and `output_repr`, both of type `Representation` as initialization parameters to your estimator class. Provide the desired default values for both.\n",
    "- Override `train()` from the `GluonEstimator` base class to initialize both representations using the training data, i.e. call `initialize_from_dataset(training_data)` on both representations.\n",
    "- Pass both representations to the training and prediction networks as initialization parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gluonts.model.estimator import GluonEstimator\n",
    "from gluonts.model.predictor import Predictor, RepresentableBlockPredictor\n",
    "from gluonts.core.component import validated\n",
    "from gluonts.support.util import copy_parameters\n",
    "from gluonts.transform import ExpectedNumInstanceSampler, Transformation, InstanceSplitter\n",
    "from gluonts.dataset.field_names import FieldName\n",
    "from mxnet.gluon import HybridBlock\n",
    "\n",
    "class MyEstimator(GluonEstimator):\n",
    "    \n",
    "    @validated()\n",
    "    def __init__(\n",
    "        self,\n",
    "        freq: str,\n",
    "        context_length: int,\n",
    "        prediction_length: int,\n",
    "        input_repr: Representation = RepresentationChain(\n",
    "            chain=[MedianScaling(), DimExpansion()]\n",
    "        ),\n",
    "        output_repr: Representation = MedianScaling(),\n",
    "        trainer: Trainer = Trainer()\n",
    "    ) -> None:\n",
    "        super().__init__(trainer=trainer)\n",
    "        self.context_length = context_length\n",
    "        self.prediction_length = prediction_length\n",
    "        self.freq = freq\n",
    "        self.input_repr = input_repr\n",
    "        self.output_repr = output_repr\n",
    "        \n",
    "    def train(\n",
    "        self,\n",
    "        training_data,\n",
    "        **kwargs,\n",
    "    ) -> Predictor:\n",
    "        self.input_repr.initialize_from_dataset(training_data)\n",
    "        self.output_repr.initialize_from_dataset(training_data)\n",
    "        return super().train(training_data, **kwargs)\n",
    "\n",
    "    def create_transformation(self):\n",
    "        return InstanceSplitter(\n",
    "            target_field=FieldName.TARGET,\n",
    "            is_pad_field=FieldName.IS_PAD,\n",
    "            start_field=FieldName.START,\n",
    "            forecast_start_field=FieldName.FORECAST_START,\n",
    "            train_sampler=ExpectedNumInstanceSampler(num_instances=1),\n",
    "            past_length=self.context_length,\n",
    "            future_length=self.prediction_length,\n",
    "        )\n",
    "\n",
    "    def create_training_network(self) -> MyTrainNetwork:\n",
    "        return MyTrainNetwork(\n",
    "            prediction_length=self.prediction_length,\n",
    "            input_repr=self.input_repr,\n",
    "            output_repr=self.output_repr\n",
    "        )\n",
    "\n",
    "    def create_predictor(\n",
    "        self, transformation: Transformation, trained_network: HybridBlock\n",
    "    ) -> Predictor:\n",
    "        prediction_network = MyPredNetwork(\n",
    "            prediction_length=self.prediction_length,\n",
    "            input_repr=self.input_repr,\n",
    "            output_repr=self.output_repr\n",
    "        )\n",
    "\n",
    "        copy_parameters(trained_network, prediction_network)\n",
    "\n",
    "        return RepresentableBlockPredictor(\n",
    "            input_transform=transformation,\n",
    "            prediction_net=prediction_network,\n",
    "            batch_size=self.trainer.batch_size,\n",
    "            freq=self.freq,\n",
    "            prediction_length=self.prediction_length,\n",
    "            ctx=self.trainer.ctx,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train `MyEstimator` with `MedianScaling`\n",
    "\n",
    "We can now train our simple `MyEstimator` which defaults to `MedianScaling` on both the input and the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "estimator = MyEstimator(\n",
    "    prediction_length=dataset.metadata.prediction_length,\n",
    "    context_length=100,\n",
    "    freq=dataset.metadata.freq,\n",
    "    trainer=Trainer(ctx=\"gpu\",\n",
    "        epochs=10,\n",
    "        learning_rate=1e-3,\n",
    "        num_batches_per_epoch=100,\n",
    "        hybridize=False\n",
    "    )\n",
    ")\n",
    "\n",
    "predictor = estimator.train(dataset.train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate evaluation predictions & evaluate\n",
    "\n",
    "Standard code for generating and evaluating predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_it, ts_it = make_evaluation_predictions(\n",
    "    dataset=dataset.test,\n",
    "    predictor=predictor,\n",
    "    num_samples=100,\n",
    ")\n",
    "\n",
    "forecasts = list(forecast_it)\n",
    "tss = list(ts_it)\n",
    "evaluator = Evaluator(quantiles=[0.1, 0.5, 0.9])\n",
    "agg_metrics, item_metrics = evaluator(iter(tss), iter(forecasts), num_series=len(dataset.test))\n",
    "print(json.dumps(agg_metrics, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with scales in distributions\n",
    "\n",
    "In some output distributions, scaling via the mean scale is already incorporated via the `TransformedDistribution`. Therefore, if sampling from the chosen ouput distribution already returns samples from the original scale, you should supply `Representation()` as the model's `output_repr`. For example, `DeepAREstimator`, `SimpleFeedForwardEstimator`, and `TransformerEstimator` resort to explicit input mean scaling but implicit output scaling via `StudenTOutput()`, which is why the input representation is, as expected, `RepresentationChain(chain=[MeanScaling(), DimExpansion()])` but the output distribution is `Representation()` with scaling still occurring in the output via the distribution. If you wish to surpress automatic scaling in the distribution output, return a `None` scale in the input representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with auto-regressive models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autoregressive models require us to further compute input and output representations in every auto-regressive step. In order to allow for this, I/O representations must be supplied with a fixed `scale` (this is needed by almost all representations) and a fixed `repr_params` list (this is needed for example by `LocalAbsoluteBinning`). Both of these parameters should be computed in the first forward pass during prediciton time. It is important for these parameters to stay fixed and not to be re-calculated in every step as preditions might diverge if the parameters can vary. Consult the DeepAR `_network.py` implementation for a complete example. See a short conceptual example below:\n",
    "\n",
    "```python\n",
    "# Initial forward pass\n",
    "input_tar_repr, scale, rep_params_in = self.input_repr(\n",
    "    sequence, sequence_obs, None, []\n",
    ")\n",
    "_, _, rep_params_out = self.output_repr(\n",
    "    past_target, past_observed_values, None, []\n",
    ")\n",
    "\n",
    "# Compute initial repeated_past_target and repeated_scale\n",
    "...\n",
    "\n",
    "# Autoregressive loop\n",
    "for (...):\n",
    "    input_tar_repr, _, _ = self.input_repr(\n",
    "        repeated_past_target,\n",
    "        F.ones_like(repeated_past_target),\n",
    "        repeated_scale,\n",
    "        rep_params_in,\n",
    "    )\n",
    "    _, _, rep_params = self.output_repr(\n",
    "        repeated_past_target,\n",
    "        F.ones_like(repeated_past_target),\n",
    "        repeated_scale,\n",
    "        rep_params_out,\n",
    "    )\n",
    "    \n",
    "    # Propagate input through network\n",
    "    ...\n",
    "    \n",
    "    # Obtain samples from distribution\n",
    "    new_samples = distr.sample()\n",
    "\n",
    "    # Post transform the data\n",
    "    new_samples = self.output_repr.post_transform(\n",
    "        F, new_samples, repeated_scale, rep_params\n",
    "    )\n",
    "    \n",
    "    # Update repeated_past_target to include new samples\n",
    "    ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "repr_venv",
   "language": "python",
   "name": "repr_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
