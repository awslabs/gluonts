# Loading data with Pandas


If you have data in Pandas ``Series`` or ``DataFrame`` objects, it is easy
to feed it to GluonTS models for training or prediction: this can be done using

```pandas
from gluonts.dataset.pandas import PandasDataset
```

Here is a summary of how you can use this class:
* You can construct it with a collection of ``Series`` or ``DataFrame`` objects,
Each of which will correspond to an entry (a series) of the dataset. The collection
can be a Python dictionary, in which case keys are used as ``item_id`` for the corresponding
dataset element.
* When entries are given as ``DataFrame`` objects, one can specify which column
contains time information (otherwise the index will be used), and should specify
which column contains the target data (the series to be modeled). Optionally, one
can also provide column names for dynamic (numerical) features.
* If each dataset entry (series) has associated static features (non time depentent),
both numerical and categorical, they can be provided as a separate ``DataFrame`` whose
index contains ``item_id`` values.
* If all data for all series is in a single long-format ``DataFrame``, you can use the
``PandasDataset.from_long_dataframe`` constructor, and specify which column contains
the ``item_id`` (data will be grouped by its values).

Let's see examples of how to use this with common data layouts.

## Example: wide data format

Here, data is given in a data frame where each column corresponds to a different series.
For example, each column could represent a different product, and contain features of the
product as well as sales data over time.

We are going to generate random data as follows, but you may get a similar ``DataFrame``
out of a CSV file. For example, a similar structure is found in the
[data for the M5 competition](https://www.kaggle.com/competitions/m5-forecasting-uncertainty/data).

```python
import pandas as pd
import numpy as np

def generate_wide_dataframe(num_series=100, series_length=1000):
    timestamps = pd.period_range(
        "2015-08-01", freq="D", periods=series_length
    ).map(str).tolist()
    columns = {
        "feature_name": ["color", "height"] + timestamps
    }
    for k in range(num_series):
        n = np.random.randint(low=1, high=10)
        p = np.random.uniform(low=0.01, high=1.0)
        columns[f"item_{k}"] = [
            np.random.choice(["red", "green", "blue"]),
            np.random.normal(loc=100, scale=15),
        ] + list(np.random.negative_binomial(n=n, p=p, size=series_length))
    return pd.DataFrame.from_dict(columns)

df_wide = generate_wide_dataframe()
```

Here is what the data looks like:

```python
print(df_wide.head())
```

First we want to separate static (first two rows) from dynamic data.
Static features are in the first two rows, which we transpose and assign
a proper type to, to mark them as numerical/categorical:

```python
df_static_features = df_wide.iloc[:2].set_index("feature_name").rename_axis(None).T
df_static_features = df_static_features.astype({"color": "category", "height": float})
```

The other rows contain the series data that we want to model:

```python
df_series_values = df_wide.iloc[2:].set_index("feature_name").rename_axis(None)
```

We can now split the columns into independent ``pd.Series`` using `dict`,
and construct the dataset:

```
dataset = PandasDataset(
    dict(df_series_values),
    static_features=df_static_features,
)
```

That's it! If we print the ``dataset`` object, we can verify its size (number of entries),
the number of features each entry has, the cardinality of the categorical features:

```
print(dataset)
```

## Example: long data format

Here, data from multiple series is interleaved in the same column, and an `item_id` column
is used to distinguish between different series. This could be the case when data is collected
in real time and appended sequentially to a CSV file, or some other format.

The following generates the same data as above, in such a "long" layout.

```python
def generate_long_dataframe(num_series=100, series_length=1000):
    colors = np.random.choice(["red", "green", "blue"], size=num_series)
    heights = np.random.normal(loc=100, scale=15, size=num_series)
    ns = np.random.randint(low=1, high=10, size=num_series)
    ps = np.random.uniform(low=0.01, high=1.0, size=num_series)
    values = np.random.negative_binomial(
        n=np.broadcast_to(ns, (series_length, num_series)),
        p=np.broadcast_to(ps, (series_length, num_series)),
    )
    timestamps = pd.period_range(
        "2015-08-01", freq="D", periods=series_length
    ).map(str).tolist()
    rows = []
    for j, ts in enumerate(timestamps):
        for k in range(num_series):
            rows.append({
                "timestamp": ts,
                "item_id": f"item_{k}",
                "color": colors[k],
                "height": heights[k],
                "target": values[j, k],
            })
    df = pd.DataFrame.from_records(rows)
    return df

df_long = generate_long_dataframe()
```

Here is what the data looks like:

```python
print(df_long.head())
```

In this case there's little manipulation needed: we just need to mark the
``"color"`` column as categorical

```python
df_long["color"] = df_long["color"].astype("category")
```

and invoke the `from_long_dataframe` constructor

```python
dataset = PandasDataset.from_long_dataframe(
    df_long,
    item_id="item_id",
    timestamp="timestamp",
    freq="D",
    static_feature_columns=["color", "height"],
)
```

We are specifying what the `item_id` column is, and what columns contain timestamp
information and static features. Like before, printing out the dataset object
gives us information about its composition:

```
print(dataset)
```
