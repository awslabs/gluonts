{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to run a custom script on SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Gluonts is currently lacking an Experimentation Framework that is easy to use, supports execution on Sagemaker, and allows for easy configuration of reproducible experiments.***<br/>\n",
    "***For this reason the GluonTS SageMaker SDK was created building on the Amazon Sagemaker Pythond SDK.***<br/>\n",
    "***In this how-to tutorial we will write a script where we train a SimpleFeedForwardEstimator on the m4_hourly dataset located in our s3 bucket on AWS Sagemaker using the GluonTSFramework, and later evaluate its performance.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import gluonts\n",
    "from gluonts.nursery.sagemaker_sdk.estimator import GluonTSFramework\n",
    "from gluonts.model.simple_feedforward import SimpleFeedForwardEstimator\n",
    "from gluonts.trainer import Trainer\n",
    "import tempfile\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, you should define where all the files generated during the experiment (model artifacts, result files, other custom scripts and dependencies used for the experiment) will be saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_dir = \"<your_s3_bucket>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we want to run experiments on Sagemaker, we need to create a sagemaker session with our AWS credentials.<br/>\n",
    "Here we use the \"default\" profile (see [boto3](https://boto3.amazonaws.com/v1/documentation/api/latest/guide/quickstart.html#using-boto-3)) and the \"us-west-2\" region (where our specified s3 bucket has to be!).<br/>\n",
    "We also need to provide an AWS IAM role (see [IAM](https://docs.aws.amazon.com/IAM/latest/UserGuide/introduction.html)) to with which to access the resources on our account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_region=\"<your_region>\"\n",
    "boto_session = boto3.session.Session(profile_name=\"<default or your_profile>\", region_name=my_region)\n",
    "sagemaker_session =  sagemaker.session.Session(boto_session=boto_session)\n",
    "role = '<your_aws_iam_role>'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now we have to also specify an own image hosted on ECR (see [ECR](https://docs.aws.amazon.com/AmazonECR/latest/userguide/docker-basics.html)) that we want to run our experiment in. <br/>\n",
    "// You can use one of the provided Docker files to create the appropriate image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docker_image = \"<your_ecr_dorcker_image_path>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We give our training job a base name that reflects the overall sentiment of the experiments that we are about to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_job_description = \"<your_experiment_007>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, since we want to train on a custom dataset, we have to specify its location, \n",
    "which has to ne on s3 for now. We will dub this dataset \"my_dataset\" which is relevant when\n",
    "we write our custom script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_s3_dataset = \"<s3_location_of_your_dataset/m4_hourly/>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_inputs = {'my_dataset': sagemaker.session.s3_input(my_s3_dataset, content_type='application/json')} # at least one required"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, we can specify any dependencies. For now we will only specify a specific gluonts version using git+ and a specific hash. # Be careful to include the dependencies of that gluonts version either in the docker image that you use or in the requirements.txt too. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "requirements_dot_txt_file_name = \"requirements.txt\"\n",
    "requirements_dot_txt_file_content = \"\"\"\n",
    "git+https://github.com/awslabs/gluon-ts.git@b9ee9cbc9d6212040fd4be21a460a048e7188306\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can define our custom script that we want to run on sagemaker: in this case we will only train a SimpleFeedForwardEstimator on our dataset located in s3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entrypoint_dot_py_file_name = \"my_entrypoint.py\"\n",
    "entrypoint_dot_py_file_content = \"\"\"\n",
    "# Standard library imports\n",
    "import argparse\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "# First-party imports\n",
    "from gluonts.dataset import common\n",
    "from gluonts.dataset.repository import datasets # for the built in gluonts dataset\n",
    "from gluonts.evaluation import Evaluator, backtest\n",
    "from gluonts.model.simple_feedforward import SimpleFeedForwardEstimator\n",
    "from gluonts.trainer import Trainer\n",
    "\n",
    "# Logging: print logs analogously to Sagemaker.\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def run(arguments):\n",
    "    logger.info(\"Starting - started running custom script.\")\n",
    "\n",
    "    # We will add a hyperparameter called \"num_samples\" later\n",
    "    # Note that we have to select hyperparameters from a dict called sm_hp\n",
    "    samples = int(arguments.sm_hps[\"num_samples\"])\n",
    "\n",
    "    # define estimator\n",
    "    my_estimator = SimpleFeedForwardEstimator(\n",
    "                        prediction_length=48,\n",
    "                        freq=\"H\",\n",
    "                        trainer=Trainer(ctx=\"cpu\") # optional\n",
    "                    )\n",
    "\n",
    "    # load custom dataset in gluonts format\n",
    "    s3_dataset_dir = Path(arguments.my_dataset)\n",
    "    dataset = common.load_datasets(\n",
    "        metadata=s3_dataset_dir,\n",
    "        train=s3_dataset_dir / \"train\",\n",
    "        test=s3_dataset_dir / \"test\",\n",
    "    )\n",
    "\n",
    "    # train our model\n",
    "    predictor = my_estimator.train(dataset.train)\n",
    "    forecast_it, ts_it = backtest.make_evaluation_predictions(\n",
    "        dataset=dataset.test,\n",
    "        predictor=predictor,\n",
    "        num_samples=samples,\n",
    "    )\n",
    "    \n",
    "    # evaluate our model\n",
    "    evaluator = Evaluator()\n",
    "    agg_metrics, item_metrics = evaluator(\n",
    "        ts_it, forecast_it, num_series=len(dataset.test)\n",
    "    )\n",
    "    \n",
    "    # anything saved to output_data_dir will be copied back to s3\n",
    "    output_dir = Path(arguments.output_data_dir)\n",
    "    with open(output_dir / \"agg_metrics.json\", \"w\") as f:\n",
    "        json.dump(agg_metrics, f)\n",
    "    \n",
    "    # model has special folder, which will be zipped and copied back to s3\n",
    "    model_output_dir = Path(arguments.model_dir) \n",
    "    predictor.serialize(model_output_dir)\n",
    "    \n",
    "    # log the metrics\n",
    "    logger.info(str(agg_metrics))\n",
    "    logger.info(str(ts_it))\n",
    "\n",
    "    return\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # load hyperparameters via SM_HPS environment variable\n",
    "    parser.add_argument(\n",
    "        \"--sm_hps\", type=json.loads, default=os.environ[\"SM_HPS\"]\n",
    "    )\n",
    "\n",
    "    # save your model here to deploy it to an endpoint later with deploy()\n",
    "    parser.add_argument(\n",
    "        \"--model_dir\", type=str, default=os.environ[\"SM_MODEL_DIR\"]\n",
    "    )\n",
    "    # specified inputs (input channels) are saved here\n",
    "    parser.add_argument(\n",
    "        \"--input_dir\", type=str, default=os.environ[\"SM_INPUT_DIR\"]\n",
    "    )\n",
    "    # contents of this folder will be written back to s3\n",
    "    parser.add_argument(\n",
    "        \"--output_data_dir\", type=str, default=os.environ[\"SM_OUTPUT_DATA_DIR\"]\n",
    "    )\n",
    "\n",
    "    # because we add my_dataset in inputs:\n",
    "    parser.add_argument('--my_dataset', type=str, default=os.environ['SM_CHANNEL_MY_DATASET']) \n",
    "\n",
    "    args, _ = parser.parse_known_args()\n",
    "\n",
    "    run(args)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we create the temporary files. Ideally you would just have a \"requirements.txt\" and \"entrypoint.py\" in your directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only using temporary directory for demonstration\n",
    "temp_dir = Path(tempfile.mkdtemp())\n",
    "\n",
    "# create the requirements.txt file\n",
    "with open(temp_dir / requirements_dot_txt_file_name, \"w\") as req_file: # has to be called requirements.txt\n",
    "    req_file.write(requirements_dot_txt_file_content)\n",
    "my_requirements_txt_file = str(temp_dir / requirements_dot_txt_file_name)\n",
    "\n",
    "# create the entrypoint.py file\n",
    "with open(temp_dir / entrypoint_dot_py_file_name, \"w\") as entry_file: # has to be called requirements.txt\n",
    "    entry_file.write(entrypoint_dot_py_file_content)\n",
    "entrypoint_dot_py_file = str(temp_dir / entrypoint_dot_py_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have everything defined we can finally run our experiment. Note that here we have to define the hyperparameters we want to use in out script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_experiment, my_job_name = GluonTSFramework.run(\n",
    "                    entry_point=entrypoint_dot_py_file,\n",
    "                    inputs = my_inputs,\n",
    "                    sagemaker_session=sagemaker_session,\n",
    "                    role=role,\n",
    "                    image_name=docker_image,  \n",
    "                    base_job_name=base_job_description,\n",
    "                    train_instance_type=\"ml.c5.xlarge\", # CPU instance. If you use a GPU image, use a GPU instance here.\n",
    "                    output_path=experiment_dir, # optional.\n",
    "                    code_location=experiment_dir, # optional.\n",
    "                    dependencies=[my_requirements_txt_file], # or the currently imported one [Path(gluonts.__path__[0])]\n",
    "                    hyperparameters={\"num_samples\":150} # optional\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we could head to our bucket to download the model artifacts and anything else we saved into the output dir, which will be located here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{experiment_dir}/{my_job_name}/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}