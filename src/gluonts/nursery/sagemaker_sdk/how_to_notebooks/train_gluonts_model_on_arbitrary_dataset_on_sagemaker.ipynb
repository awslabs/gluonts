{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to train any GluonTS model on any dataset on SageMaker:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Gluonts is currently lacking an Experimentation Framework that is easy to use, supports execution on Sagemaker, and allows for easy configuration of reporoducible experiments.***<br/>\n",
    "***For this reason the GluonTS SageMaker SDK was created building on the Amazon Sagemaker Pythond SDK.***<br/>\n",
    "***In this how-to tutorial we will train a SimpleFeedForwardEstimator on the m4_hourly dataset on AWS Sagemaker using the GluonTSFramework, and later evaluate its performance.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import gluonts\n",
    "from gluonts.sagemaker.estimator import GluonTSFramework\n",
    "from gluonts.model.simple_feedforward import SimpleFeedForwardEstimator\n",
    "from gluonts.trainer import Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, you should define where all the files generated during the experiment (model artifacts, result files, other custom scripts and dependencies used for the experiment) will be saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_dir = \"<your_s3_bucket>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we want to run experiments on Sagemaker, we need to create a sagemaker session with our AWS credentials.<br/>\n",
    "Here we use the \"default\" profile (see [boto3](https://boto3.amazonaws.com/v1/documentation/api/latest/guide/quickstart.html#using-boto-3)) and the \"us-west-2\" region (where our specified s3 bucket has to be!).<br/>\n",
    "We also need to provide an AWS IAM role (see [IAM](https://docs.aws.amazon.com/IAM/latest/UserGuide/introduction.html)) to with which to access the resources on our account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_region=\"<your_region>\"\n",
    "boto_session = boto3.session.Session(profile_name=\"<default or your_profile>\", region_name=my_region)\n",
    "sagemaker_session =  sagemaker.session.Session(boto_session=boto_session)\n",
    "role = '<your_aws_iam_role>'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now we have to also specify an own image hosted on ECR (see [ECR](https://docs.aws.amazon.com/AmazonECR/latest/userguide/docker-basics.html)) that we want to run our experiment in. <br/>\n",
    "// You can use one of the provided Docker files to create the appropriate image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docker_image = \"<your_ecr_dorcker_image_path>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We give our training job a base name that reflects the overall sentiment of the experiments that we are about to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_job_description = \"<your_experiment_007>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create the experimentation framework for the train job.<br/>\n",
    "Here we have to decide on the instance type we want to run our experiments on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_experiment = GluonTSFramework(\n",
    "                    sagemaker_session=sagemaker_session,\n",
    "                    role=role,\n",
    "                    image_name=docker_image,  \n",
    "                    base_job_name=base_job_description,\n",
    "                    train_instance_type=\"ml.c5.xlarge\", # CPU instance. If you use a GPU image, use a GPU instance here.\n",
    "                    output_path=experiment_dir, # optional.\n",
    "                    code_location=experiment_dir, # optional.\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In GluonTSFramework you can specify: \"dependencies=[my_specific_gluonts_version_path]\" as a parameter, with any specific gluonts version you would like your experiments to run in. For this you will have to use a docker image that has all the corresponding requirements installed, but not gluonts itself, as dependencies are only appended to the sys.path. # will be fixed very soon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define the Estimator we want to train, which can be any GluonEstimator with any hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_estimator = SimpleFeedForwardEstimator(\n",
    "                    prediction_length=48,\n",
    "                    freq=\"H\",\n",
    "                    trainer=Trainer(ctx=\"cpu\") # optional\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally we call the *train* method to train our estimator, where we just specify our dataset and estimator. <br/>\n",
    "The dataset can be either a built in one provided by gluonts: *gluonts.dataset.repository.datasets.dataset_recipes.keys()* or any dataset in the gluonts dataset format located on s3: <br/>\n",
    ">dataset<br/>\n",
    ">  ├-> train<br/>\n",
    ">  |   └> data.json<br/>\n",
    ">  ├-> test<br/>\n",
    ">  |   └> data.json<br/>\n",
    ">  └> metadata.json<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_metrics, item_metrics, job_name = my_experiment.train(dataset=\"m4_hourly\", estimator=my_estimator) \n",
    "#agg_metrics, item_metrics, job_name = my_experiment.train(dataset=\"s3://<you_s3_dataset_path>\", estimator=my_estimator) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can inspect our training progress and monitored metrics (like resource consumption or epoch loss) on Sagemaker under \"Training/Training jobs\" here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"https://{my_region}.console.aws.amazon.com/sagemaker/home?region={my_region}#/jobs/{job_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or look at our results right here when our training job finished:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or head to our bucket to download the model artifacts, which will be located here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f\"{experiment_dir}/{job_name}/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
